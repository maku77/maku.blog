---
title: "機械学習 (ML) 用語のまとめ"
url: "p/ouzjm6o/"
date: "2023-12-30"
tags: ["ml"]
draft: true
---

{{% private %}}
コスト関数 (cost function)

エラー率 (error rate)

価値関数 (value function)

汎化誤差 (generation error)、サンプル外エラー (out-of-sample error)
: 見たことのないデータをうまく予測できるかどうかということ。

ホールドアウトセット (holdout set)
: ホールドアウトセットは、訓練データから一部を取り分けておき、モデルの訓練が終了した後にその性能を評価するために用いられるデータセットです。
holdout は「取り分けておく」という意味です。
通常、訓練データ全体の70〜80％をモデルの訓練に使用し、残りの部分をホールドアウトセットとして保持します。
モデルの調整段階で使用する検証セット (validation set) と、最終的な性能評価に使用するテストセット (test set) があります。

テストセット (test set)
: テストセットは、最終的なモデルの評価に用いられるデータセットです。
モデルの訓練や検証には使用せず、訓練データから独立したデータセットをテストセットとして保持します。

検証セット (validation set)
: 検証セットは、モデルのハイパーパラメータの調整や性能評価のために使用される別のデータセットです。
訓練データを分割した一部を検証セットとして取り分けます。
{{% /private %}}


アンサンブル法
: ある問題を解決するために複数のモデルを組み合わせて使用する方法。
「バギング法（平均法）」では複数の推定器の出力の平均を取り、「ブースティング法」では、ある推定器の出力を他の推定器によって修正します。
例えば、ランダムフォレスト（複数の決定木の組み合わせ）や、Adaptive Boosting (AdaBoost) などがあります。

特徴量エンジニアリング
: 読み込んだデータを機械学習用アルゴリズムが使える形に加工すること。あるいは、既存のデータの組み合わせや加工により、新しい特徴量を作成すること。

標準化
: 各特徴量の平均を 0、標準偏差を 1 になるように揃えること。
scikit-learn であれば、`sklearn.preprocessing.StandardScaler()` を使用して標準化を行えます。
伝統的な機械学習アルゴリズムでは特徴量の標準化を行わないと学習がうまくいかないことがありますが、「ランダムフォレスト」や「LightGBM」などの標準化が必要ないアルゴリズムもあります。

カテゴリ変数／カテゴリカル変数／質的変数
: カテゴリ変数は、数値ではなく特定のカテゴリや群を表す変数です。
例えば、血液型（A型、B型、O型、AB型）、色（赤、青、緑）、商品のカテゴリ（服、食品、家電）などがカテゴリ変数の例です。
一般的には文字列で表現されるもので、数値にマッピングすることは可能ですが、それらの数値の大小関係に意味はありません。

訓練セット
: 訓練用のデータの集合。

訓練サンプル、訓練インスタンス
: 訓練用の個々のデータ。

入力変数
: 特徴量 (feature)、予測変数 (predictor)、独立変数 (independent variable) とも。

出力変数
: 目標（目的）変数 (target variable)、従属変数 (dependent variable)、応答変数 (response variable) とも。
クラス分類の場合は、「変数」ではなく「クラス」と呼ばれることもあります。

正解ラベル
: モデルが予測しようとする対象の出力または目的変数を表します。
通常、教師あり学習（Supervised Learning）の文脈で使われます。
正解ラベルは、トレーニングデータセットにおいて各入力データに対して対応する出力や目的となる値です。
例えば、画像が犬か猫かを分類するモデルを考えると、各画像データ（入力）に対してその画像が犬か猫かのラベル（出力）が正解ラベルとなります。

データの水増し (Data Augmentation)
: 既存の訓練データを変換・拡張することで、データの多様性を増やすプロセスです。
例えば、画像分類用の機械学習のために、画像データを加工することによってデータを水増しします。
データの水増しには次のような手法が用いられます。

  - 画像の変形: 画像を回転、拡大縮小、反転、トリミングなどの方法で変形させます。
  - 色の変更: 画像の色調や明るさを変更することで、光の条件や環境の変化に対するロバストなモデルを訓練できます。
  - ノイズの追加: 画像やデータにノイズを追加することで、モデルがノイズに対して頑健な性能を持つようになります。
  - 幾何学的な変換: 幾何学的な変換（歪み、ずれ、変形）を適用して、物体の位置や形状を変化させます。

データラングリング (data wrangling)
: 生のデータからごみを取り去り、扱いやすい形式に変換すること。

決定境界
: 分類アルゴリウムがデータを各クラスに分類するときの境界。

物体検出モデル／物体検知モデル
: 「画像のどの領域に物体があるか」と「その物体は何か」を予測するためのモデル。

バウンディングボックス (Bounding Box)
: 画像内の物体が写っていると思われる領域を囲んだ矩形。
2 つのバウンディングボックスの大部分が重なっている場合、1 つの物体に対して 2 つのバウンディングボックスが割り当てられている可能性が高いため、それぞれの「信頼度スコア」を比べて低い方を破棄します。
ちなみに、このときの重複面積の割合のことを __IoU: Intersection over Union__ と呼び、閾値として 0.5 がよく使われます。

Early Stopping
: 学習モデルの過学習を防ぐため、検証用データセットに対する精度が向上しなくなった時点で学習を打ち切ること。

OVR (One Vs Rest)
: 2 値のロジスティック回帰の予測モデルを、3 クラス以上の分類に使用する手法。
例えば、クラス A/B/C の 3 値に分類する問題があるとします。
それぞれのクラスで 2 値の予測 (y=0 or y=1) を行うことで、クラス A である確率、クラス B である確率、クラス C である確率が求められます。
これらのうち、一番確率の高いものを 3 値分類の結果とするという手法です。

RNN: Recurrent Neural Network（リカレント・ニューラル・ネットワーク）
: 1 つ前のモデル出力と新しい入力の組み合わせを繰り返し使用するモデル。

