<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>機械学習／ディープラーニング on まくろぐ</title><link>https://maku.blog/p/7ipkgyk/</link><description>Recent content in 機械学習／ディープラーニング on まくろぐ</description><generator>Hugo -- gohugo.io</generator><language>ja-jp</language><lastBuildDate>Mon, 29 Apr 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://maku.blog/p/7ipkgyk/index.xml" rel="self" type="application/rss+xml"/><item><title>機械学習メモ: ROC カーブと AUC</title><link>https://maku.blog/p/ha99p76/</link><pubDate>Mon, 29 Apr 2024 00:00:00 +0000</pubDate><guid>https://maku.blog/p/ha99p76/</guid><description>ROC カーブとは ROC (Receiver Operating Characteristic) カーブ（曲線） は、二値分類モデルの性能を評価するためのグラフです。 陽性と陰性を判断するための閾値を変換させたときに、真陽性率 (TPR: True Positive Rate) と 偽陽性率 (FPR: False Positive Rate) がどのように変化するかプロットしたものです。
参考リンク TP/TN/FP/FN などの用語について → 混同行列と関連指標 真陽性率 (TPR) と 偽陽性率 (FPR) 真陽性率（＝再現率、Recall）は、陽性（正例）サンプルのうち、正しく陽性と予測できたものが何％あったかを示すもので、高いほど（1 に近いほど）よいです。
$$ TPR \left( 真陽性率 \right) = \frac{TP}{TP + FN} $$
偽陽性率は、陰性（負例）サンプルのうち、誤って陽性と予測してしまったものが何％あったかを示すもので、低いほど（0 に近いほど）よいです。
$$ FPR \left( 偽陽性率 \right) = \frac{FP}{FP + TN} $$
これらはトレードオフの関係にあり、陽性あるいは陰性と判断するための閾値を変化させたときに次のような感じで変化します。
図: 一般的な ROC 曲線 このとき、どの閾値がよいかは一概には言えなくて、真陽性率を犠牲にしてでも偽陽性率を低くしたいのであれば閾値 A を選べばよいし、真陽性率をできるだけ高めたいのであれば閾値 C を選びます。 バランスよく選びたいのであれば閾値 B を選ぶことになります。
AUC (Area Under the Curve) 二値分類モデルの性能を示すための指標として、AUC (Area Under the Curve) があります。 AUC は、ROC カーブの下の部分面積であり、大きいほど（1 に近いほど）よい性能だということを表します。 前述の通り、二値分類モデルで使用する閾値は、どの値がベストかは一概に決められないのですが、AUC を使えば、二値分類モデルの全体的な性能を客観的に示すことができます。</description></item><item><title>機械学習メモ: 混同行列と関連指標</title><link>https://maku.blog/p/9dveh3m/</link><pubDate>Sun, 21 Apr 2024 00:00:00 +0000</pubDate><guid>https://maku.blog/p/9dveh3m/</guid><description>混同行列 (confusion matrix) とは 混同行列 (confusion matrix) は、機械学習やパターン認識などの分野で使われる評価手法のひとつで、主に分類問題の性能を評価するために使用されます。 混同行列は、実際のデータが属するクラスと、モデル（分類器）が予測したクラスを比較することで、分類器の性能を可視化するのに役立ちます。
図: 混同行列 混同行列は、予測モデルが予測したデータを下記の 4 種類に分けてカウントします。 陽性・陰性というのは、例えば、「病気の有無」や「本物・偽物」といった分類を示します。
真陽性 (TP: True Positives) &amp;hellip; 陽性データのうち、モデルが正しく陽性と予測した数。 真陰性 (TN: True Negatives) &amp;hellip; 陰性データのうち、モデルが正しく陰性と予測した数。 偽陽性 (FP: False Positives) &amp;hellip; 陰性データのうち、モデルが誤って陽性と予測した数。 偽陰性 (FN: False Negatives) &amp;hellip; 陽性データのうち、モデルが誤って陰性と予測した数。 一般的には二値分類に使われますが、多クラスの分類でも利用されます。 混同行列をベースにして、分類問題の様々な評価指標を計算できます。
正答率 (accuracy) と誤答率 (error rate) 再現率 (recall) と 適合率 (precision) F1 スコア、Fβ スコア マシューズ相関係数 (MCC) ここで、正答率 (accuracy) というのは、単純に「予測が当たった数／データ数」という計算で求められるものです。 例えば、10 個のデータを分類して、5 個のデータを正しく分類できれば、正答率 (accuracy) は50％です。
でも、実世界の分類問題はこんな簡単に評価できるものではなかったりします。 典型的なのが疾患の有無の判断です。 実際に 100 人中 5 人が疾患を持っている場合、誰も疾患を持っていないと予測すれば、その予測モデルの精度は95％になります。 でもそんな予測モデルは役に立ちませんよね？ そうではなく、いかに間違いなく疾患を持っている人を見つけられるかが重要なはずです。 そこで必要になってくるのが、再現率、適合率、F1 スコアといった評価指標です。</description></item><item><title>機械学習メモ: 評価関数（コスト関数）のまとめ</title><link>https://maku.blog/p/psotadn/</link><pubDate>Tue, 06 May 2014 00:00:00 +0000</pubDate><guid>https://maku.blog/p/psotadn/</guid><description>平均絶対誤差 (MAE: Mean Absolute Error) $$ \text{MAE} = \frac{1}{n} \sum_{i=1}^{n} |y_i - \hat{y}_i| $$
\( y_i \) : 実際の値 \( \hat{y_i} \) : 予測値 NumPy での実装 mae = np.mean(np.abs(actual_values - predicted_values)) 平均二乗誤差 (MSE: Mean Squared Error) $$ \operatorname{MSE}=\frac{1}{n}\sum_{i=1}^n(y_i-\hat{y_i})^2 $$
\( y_i \) : 実際の値 \( \hat{y_i} \) : 予測値 予測値とのずれの二乗をすべて足して平均をとったもの。
平均二乗誤差平方根 (RMSE: Root Mean Squared Error) $$ \operatorname{RMSE}=\sqrt{\operatorname{MSE}}=\sqrt{\frac{1}{n}\sum_{i=1}^n(y_i-\hat{y_i})^2} $$
\( y_i \) : 実際の値 \( \hat{y_i} \) : 予測値 平均二乗誤差 (MSE: Mean Squared Error) の平方根。 実測値と予測値がどれほど異なっているかを表します。</description></item></channel></rss>