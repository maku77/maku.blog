<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>機械学習/ディープラーニング on まくろぐ</title><link>https://maku.blog/tags/ml/</link><description>Recent content in 機械学習/ディープラーニング on まくろぐ</description><generator>Hugo -- gohugo.io</generator><language>ja-jp</language><lastBuildDate>Sun, 12 May 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://maku.blog/tags/ml/index.xml" rel="self" type="application/rss+xml"/><item><title>機械学習メモ: 分類タスクの評価 ─ balanced accuracy</title><link>https://maku.blog/p/g6v9puz/</link><pubDate>Sun, 12 May 2024 00:00:00 +0000</pubDate><guid>https://maku.blog/p/g6v9puz/</guid><description>機械学習の分類タスクにおける評価指標のひとつとして balanced accuracy があります（日本語だと加重平均正答率とかかな）。 balanced accuracy は、各クラスのデータ数に偏りがある場合に正答率 (accuracy) の計算に補正を加えます。
balanced accuracy の計算方法 通常の正解率 (accuracy) の計算は次のようなシンプルなものです。
$$ \operatorname{accuracy} = \frac{正解数 (r)}{データ数 (n)} $$
例えば、10 個のデータのうち 8 個の予測に成功した場合は、accuracy は 0.8 (80%) になりますが、blanced accuracy の場合は、各クラスに属するデータの数の逆数を重みとした加重平均を取ります。
$$ \operatorname{balanced accuracy} = \frac{1}{N} \sum_{i=1}^{N} \frac{r_i}{n_i} $$
\( N \) &amp;hellip; クラス数 \( r_i \) &amp;hellip; クラス \( i \) に属すると正しく予測できたデータ数 \( n_i \) &amp;hellip; クラス \( i \) の（実際の）データ数 要するに、もともとたくさんのデータがあるクラスに属すると予測して正解したとしても、accuracy はあまり上がらないようにするということです。 逆に、少ししかデータがないクラスに属すると予測して正解すれば、accuracy は大きく上がります。 感覚としては、各クラスごとに個別に accuracy を求めてから平均するという感じです。</description></item><item><title>機械学習メモ: 多クラス分類の評価 ─ mean-f1, macro-f1, micro-f1</title><link>https://maku.blog/p/oegkg89/</link><pubDate>Sun, 12 May 2024 00:00:00 +0000</pubDate><guid>https://maku.blog/p/oegkg89/</guid><description>多クラス分類用の F1 スコア 2 クラスの分類タスク（二値分類）の代表的な評価指標として F1 スコア がありますが、多クラスの分類タスクの評価指標としては、これを拡張した次のような指標を使います。 F1 スコアの平均をとったりするのですが、どのような単位で F1 スコアを計算するかが微妙に異なります。
評価指標 説明 mean-F1 レコードごとに F1 スコアを求め、それらの平均をとります。 macro-F1 クラスごとに F1 スコアを求め、それらの平均をとります。 micro-F1 各レコードの各クラスに対する予測（陽性 or 陰性）が TP/TN/FP/FN のいずれであるかを求め、その混同行列をもとに F1 スコアを計算します。 これらの指標は、1 つのレコードに対して複数のラベルをつけるマルチラベル分類においてよく用いられます。 下記はマルチラベル分類のタスクにおいて、各種 F1 スコアを求める実装例です。 scikit-learn の f1_score() 関数の average 引数でどのロジックを用いるかを指定できます。
scikit-learn でマルチラベル分類の F1 スコアを求める from sklearn.metrics import f1_score # サンプルデータ y_true = [[0, 1, 1], # レコード1の真のラベル (2, 3) [1, 1, 1]] # レコード2の真のラベル (1, 2, 3) y_pred = [[0, 1, 0], # レコード1の予測されたラベル (2) [1, 0, 1]] # レコード2の予測されたラベル (1, 3) print(&amp;#34; mean-F1:&amp;#34;, f1_score(y_true, y_pred, average=&amp;#34;samples&amp;#34;)) # 0.</description></item><item><title>機械学習メモ: 分類タスクの評価 ─ ROC カーブと AUC</title><link>https://maku.blog/p/ha99p76/</link><pubDate>Mon, 29 Apr 2024 00:00:00 +0000</pubDate><guid>https://maku.blog/p/ha99p76/</guid><description>ROC カーブとは ROC (Receiver Operating Characteristic) カーブ（曲線） は、二値分類モデルの性能を評価するためのグラフです。 陽性と陰性を判断するための閾値を変換させたときに、真陽性率 (TPR: True Positive Rate) と 偽陽性率 (FPR: False Positive Rate) がどのように変化するかプロットしたものです。
（参考）TP/TN/FP/FN などの用語について → 混同行列と関連指標 真陽性率 (TPR) と 偽陽性率 (FPR) 真陽性率（＝再現率、Recall）は、陽性（正例）サンプルのうち、正しく陽性と予測できたものが何％あったかを示すもので、高いほど（1 に近いほど）よいです。
$$ TPR \left( 真陽性率 \right) = \frac{TP}{TP + FN} $$
偽陽性率は、陰性（負例）サンプルのうち、誤って陽性と予測してしまったものが何％あったかを示すもので、低いほど（0 に近いほど）よいです。
$$ FPR \left( 偽陽性率 \right) = \frac{FP}{FP + TN} $$
これらはトレードオフの関係にあり、陽性あるいは陰性と判断するための閾値を変化させたときに次のような感じで変化します。
図: 一般的な ROC 曲線 このとき、どの閾値がよいかは一概には言えなくて、真陽性率を犠牲にしてでも偽陽性率を低くしたいのであれば閾値 A を選べばよいし、真陽性率をできるだけ高めたいのであれば閾値 C を選びます。 バランスよく選びたいのであれば閾値 B を選ぶことになります。
AUC (Area Under the Curve) 二値分類モデルの性能を示すための指標として、AUC (Area Under the Curve) があります。 AUC は、ROC カーブの下の部分面積であり、大きいほど（1 に近いほど）よい性能だということを表します。 前述の通り、二値分類モデルで使用する閾値は、どの値がベストかは一概に決められないのですが、AUC を使えば、二値分類モデルの全体的な性能を客観的に示すことができます。</description></item><item><title>機械学習メモ: 分類タスクの評価 ─ 混同行列と 2 クラス分類の基本的な評価指標 (accuracy, error rate, recall, precision, f1-score, fβ-score)</title><link>https://maku.blog/p/9dveh3m/</link><pubDate>Sun, 21 Apr 2024 00:00:00 +0000</pubDate><guid>https://maku.blog/p/9dveh3m/</guid><description>混同行列 (confusion matrix) とは 混同行列 (confusion matrix) は、機械学習やパターン認識などの分野で使われる評価手法のひとつで、主に分類問題の性能を評価するために使用されます。 混同行列は、実際のデータが属するクラスと、モデル（分類器）が予測したクラスを比較することで、分類器の性能を可視化するのに役立ちます。
図: 混同行列 混同行列は、予測モデルが予測したデータを下記の 4 種類に分けてカウントします。 陽性・陰性というのは、例えば、「病気の有無」や「本物・偽物」といった分類を示します。
真陽性 (TP: True Positives) &amp;hellip; 陽性データのうち、モデルが正しく陽性と予測した数。 真陰性 (TN: True Negatives) &amp;hellip; 陰性データのうち、モデルが正しく陰性と予測した数。 偽陽性 (FP: False Positives) &amp;hellip; 陰性データのうち、モデルが誤って陽性と予測した数。 偽陰性 (FN: False Negatives) &amp;hellip; 陽性データのうち、モデルが誤って陰性と予測した数。 一般的には二値分類に使われますが、多クラスの分類でも利用されます。 混同行列をベースにして、分類問題の様々な評価指標を計算できます。
正答率 (accuracy) と誤答率 (error rate) 再現率 (recall) と 適合率 (precision) F1 スコア、Fβ スコア マシューズ相関係数 (MCC) ここで、正答率 (accuracy) というのは、単純に「予測が当たった数／データ数」という計算で求められるものです。 例えば、10 個のデータを分類して、5 個のデータを正しく分類できれば、正答率 (accuracy) は50％です。
でも、実世界の分類問題はこんな簡単に評価できるものではなかったりします。 典型的なのが疾患の有無の判断です。 実際に 100 人中 5 人が疾患を持っている場合、誰も疾患を持っていないと予測すれば、その予測モデルの精度は95％になります。 でもそんな予測モデルは役に立ちませんよね？ そうではなく、いかに間違いなく疾患を持っている人を見つけられるかが重要なはずです。 そこで必要になってくるのが、再現率、適合率、F1 スコアといった評価指標です。</description></item><item><title>読書メモ『Kaggleで勝つ データ分析の技術』</title><link>https://maku.blog/p/a7p6b2s/</link><pubDate>Mon, 01 Apr 2024 00:00:00 +0000</pubDate><guid>https://maku.blog/p/a7p6b2s/</guid><description>Kaggleで勝つ データ分析の技術 門脇大輔、阪田隆司、保坂桂佑、平松雄司 技術評論社 ちょっとだけ古めの本だけど、Kaggle 本の中でも評価の高いデータ分析技術本です。 たしかによくまとまってて読みやすいですね。 以下まとめメモメモ。
1. 分析コンペとは 分析コンペって何？ コンペとは 特徴量（変数、説明変数）を入力として 目的変数を予測する 学習データ（提供されるデータ）には上記の 1 と 2 が含まれ、テストデータには 1 だけが含まれる。 学習データのイメージ（入力とその答えがある） 123, 135, 74, 12, 301, 634 → 15 423, 562, 10, 44, 125, 988 → 31 904, 111, 64, 15, 877, 502 → 23 テストデータのイメージ（入力しかない） 335, 218, 66, 40, 226, 999 → ？ 590, 449, 59, 20, 633, 490 → ？ 771, 703, 12, 30, 550, 300 → ？ LB: Leaderboard 参加者はコンペ期間中にテストデータを使って予測した値を提出する。 システム側で テストデータの一部 だけを使ってスコアが計算され、Public Leaderboard として現在の順位が公開される。 コンペ期間が終了すると、テストデータの残りの部分を使ってスコアが計算され、Private Leaderboard として最終的な順位が発表される。 Shake up Public LB (Leader Board) と Private LB の順位が大きく入れ替わること。 チームマージ 他の参加者にリクエストを出してチームを組むこと。 コンペの終盤に多くのチームマージが行われることがある。 チームの人数上限は 5 名。 マージするチーム同士の合計提出回数は「1日の上限 x コンペ日数」を超えてはいけない。毎日上限まで予測値を Submit していると、チームマージできなくなるので注意。 分析コンペのプラットフォーム 主なプラットフォーム Kaggle (worldwide) 最も有名 世界中の企業、省庁、研究機関がコンペを開催 SIGNATE（日本） 日本語 日本国内の企業、省庁、研究機関がコンペを開催 TopCoder（worldwide） プログラミングコンテストのプラットフォームだが、分析コンペも開催されている カーネルコンペ 通常のコンペ &amp;hellip; 予測値を提出する。 カーネルコンペ &amp;hellip; Kernel (Notebook) に記述したコードを提出する。 Kernel 上で学習と予測の両方を実行するタイプと、予測だけを実行すればよいタイプがある（後者はモデルのバイナリをアップロードできるようになっている）。 分析コンペに参加してから終わるまで Join Competition ボタンからコンペに参加する 規約に同意する 1 日の Submit 数やチームメンバー数の上限。 Private Sharing（チームメンバー意外へのコード共有）はダメ。Kaggle の Discussion での共有は OK。 外部データの使用可否はコンペにより異なる。 データをダウンロードする 予測値を作成する ダウンロードしたデータを使ってモデルを作る テストデータに対する予測値を求める 予測値を提出する 提出サンプル（主に CSV 形式）と同じ形式の提出用ファイルを作成して Submit。１日の提出数上限に注意。 Public Leaderboard をチェックする テストデータの一部を使ってスコア計算した結果の順位が公開される。 最終予測値を選ぶ コンペの終了前に、最終評価に使う予測値を 2 つ選んで Use for Final Score にチェックを入れる。 自分で選ばないと、Public Leaderboard の中でスコアが高いものが選ばれる。 Private Leaderboard をチェックする 分析コンペが終了したら Private LB で最終順位を確認する。 多くの場合は終了と同時に発表されるが、発表までに時間がかかるものもある。 分析コンペに参加する意義 賞金、称号、ランキング データ分析の経験 データサイエンティストとの繋がり 就業機会 上位を目指すためのポイント 探索的データ分析 (EDA: Exploratory Data Analysis) まず優先すべきはデータの理解 可視化手法 棒グラフ、箱ひげ図、バイオリンプロット、散布図、折れ線グラフ ヒートマップ、ヒストグラム Q-Q プロット t-SNE、UMAP テーブルデータのコンペでは、よいデータを作れたかどうかで順位がきまる 2.</description></item><item><title>機械学習メモ: 回帰タスクの評価 ─ MAE, MSE, RMSE, MSLE, RMSLE, MAPE, 決定係数</title><link>https://maku.blog/p/psotadn/</link><pubDate>Tue, 06 May 2014 00:00:00 +0000</pubDate><guid>https://maku.blog/p/psotadn/</guid><description>機械学習における回帰モデルは、目的変数となる数値を予測するためのモデルです。 よって、回帰モデルの精度は、真の値と予測値のズレをもとに計算されます（ズレが小さいほど精度がよい）。 その計算方法としては、単純にズレの平均値を求める方法（平均絶対誤差: MAE）や、二乗値や対数値を使う方法などいろいろあります。
平均絶対誤差 (MAE: Mean Absolute Error) 平均絶対誤差 (MAE: Mean Absolute Error) は、実測値と予測値の「誤差の絶対値」をもとに算出します。
$$ \text{MAE} = \frac{1}{n} \sum_{i=1}^{n} |y_i - \hat{y}_i| $$
\( y_i \) : 実測値 \( \hat{y_i} \) : 予測値 二乗誤差に比べると、外れ値の影響が小さいのが特徴すす。
自力で計算する方法 import numpy as np import pandas as pd y_true = pd.Series([1.0, 2.0, 3.0, 4.0, 5.0]) # 実測値 y_pred = pd.Series([0.8, 2.4, 2.5, 3.8, 7.2]) # 予測値 print(np.mean(np.abs(y_true - y_pred))) #=&amp;gt; 0.7 scikit-learn の mean_absolute_error 関数を使う方法 import numpy as np import pandas as pd from sklearn.</description></item></channel></rss>