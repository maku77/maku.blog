<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>機械学習/ディープラーニング on まくろぐ</title><link>https://maku.blog/tags/ml/</link><description>Recent content in 機械学習/ディープラーニング on まくろぐ</description><generator>Hugo -- gohugo.io</generator><language>ja-jp</language><lastBuildDate>Sun, 12 May 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://maku.blog/tags/ml/index.xml" rel="self" type="application/rss+xml"/><item><title>機械学習メモ: 分類タスクの評価 ─ balanced accuracy</title><link>https://maku.blog/p/g6v9puz/</link><pubDate>Sun, 12 May 2024 00:00:00 +0000</pubDate><guid>https://maku.blog/p/g6v9puz/</guid><description>機械学習の分類タスクにおける評価指標のひとつとして balanced accuracy があります（日本語だと加重平均正答率とかかな）。 balanced accuracy は、各クラスのデータ数に偏りがある場合に正答率 (accuracy) の計算に補正を加えます。
balanced accuracy の計算方法 通常の正解率 (accuracy) の計算は次のようなシンプルなものです。
$$ \operatorname{accuracy} = \frac{正解数 (r)}{データ数 (n)} $$
例えば、10 個のデータのうち 8 個の予測に成功した場合は、accuracy は 0.8 (80%) になりますが、blanced accuracy の場合は、各クラスに属するデータの数の逆数を重みとした加重平均を取ります。
$$ \operatorname{balanced accuracy} = \frac{1}{N} \sum_{i=1}^{N} \frac{r_i}{n_i} $$
\( N \) &amp;hellip; クラス数 \( r_i \) &amp;hellip; クラス \( i \) に属すると正しく予測できたデータ数 \( n_i \) &amp;hellip; クラス \( i \) の（実際の）データ数 要するに、もともとたくさんのデータがあるクラスに属すると予測して正解したとしても、accuracy はあまり上がらないようにするということです。 逆に、少ししかデータがないクラスに属すると予測して正解すれば、accuracy は大きく上がります。 感覚としては、各クラスごとに個別に accuracy を求めてから平均するという感じです。</description></item><item><title>機械学習メモ: 多クラス分類の評価 ─ mean-f1, macro-f1, micro-f1</title><link>https://maku.blog/p/oegkg89/</link><pubDate>Sun, 12 May 2024 00:00:00 +0000</pubDate><guid>https://maku.blog/p/oegkg89/</guid><description>多クラス分類用の F1 スコア 2 クラスの分類タスク（二値分類）の代表的な評価指標として F1 スコア がありますが、多クラスの分類タスクの評価指標としては、これを拡張した次のような指標を使います。 F1 スコアの平均をとったりするのですが、どのような単位で F1 スコアを計算するかが微妙に異なります。
評価指標 説明 mean-F1 レコードごとに F1 スコアを求め、それらの平均をとります。 macro-F1 クラスごとに F1 スコアを求め、それらの平均をとります。 micro-F1 各レコードの各クラスに対する予測（陽性 or 陰性）が TP/TN/FP/FN のいずれであるかを求め、その混同行列をもとに F1 スコアを計算します。 これらの指標は、1 つのレコードに対して複数のラベルをつけるマルチラベル分類においてよく用いられます。 下記はマルチラベル分類のタスクにおいて、各種 F1 スコアを求める実装例です。 scikit-learn の f1_score() 関数の average 引数でどのロジックを用いるかを指定できます。
scikit-learn でマルチラベル分類の F1 スコアを求める from sklearn.metrics import f1_score # サンプルデータ y_true = [[0, 1, 1], # レコード1の真のラベル (2, 3) [1, 1, 1]] # レコード2の真のラベル (1, 2, 3) y_pred = [[0, 1, 0], # レコード1の予測されたラベル (2) [1, 0, 1]] # レコード2の予測されたラベル (1, 3) print(&amp;#34; mean-F1:&amp;#34;, f1_score(y_true, y_pred, average=&amp;#34;samples&amp;#34;)) # 0.</description></item><item><title>機械学習メモ: 分類タスクの評価 ─ ROC カーブと AUC</title><link>https://maku.blog/p/ha99p76/</link><pubDate>Mon, 29 Apr 2024 00:00:00 +0000</pubDate><guid>https://maku.blog/p/ha99p76/</guid><description>ROC カーブとは ROC (Receiver Operating Characteristic) カーブ（曲線） は、二値分類モデルの性能を評価するためのグラフです。 陽性と陰性を判断するための閾値を変換させたときに、真陽性率 (TPR: True Positive Rate) と 偽陽性率 (FPR: False Positive Rate) がどのように変化するかプロットしたものです。
（参考）TP/TN/FP/FN などの用語について → 混同行列と関連指標 真陽性率 (TPR) と 偽陽性率 (FPR) 真陽性率（＝再現率、Recall）は、陽性（正例）サンプルのうち、正しく陽性と予測できたものが何％あったかを示すもので、高いほど（1 に近いほど）よいです。
$$ TPR \left( 真陽性率 \right) = \frac{TP}{TP + FN} $$
偽陽性率は、陰性（負例）サンプルのうち、誤って陽性と予測してしまったものが何％あったかを示すもので、低いほど（0 に近いほど）よいです。
$$ FPR \left( 偽陽性率 \right) = \frac{FP}{FP + TN} $$
これらはトレードオフの関係にあり、陽性あるいは陰性と判断するための閾値を変化させたときに次のような感じで変化します。
図: 一般的な ROC 曲線 このとき、どの閾値がよいかは一概には言えなくて、真陽性率を犠牲にしてでも偽陽性率を低くしたいのであれば閾値 A を選べばよいし、真陽性率をできるだけ高めたいのであれば閾値 C を選びます。 バランスよく選びたいのであれば閾値 B を選ぶことになります。
AUC (Area Under the Curve) 二値分類モデルの性能を示すための指標として、AUC (Area Under the Curve) があります。 AUC は、ROC カーブの下の部分面積であり、大きいほど（1 に近いほど）よい性能だということを表します。 前述の通り、二値分類モデルで使用する閾値は、どの値がベストかは一概に決められないのですが、AUC を使えば、二値分類モデルの全体的な性能を客観的に示すことができます。</description></item><item><title>機械学習メモ: 分類タスクの評価 ─ 混同行列と 2 クラス分類の基本的な評価指標 (accuracy, error rate, recall, precision, f1-score, fβ-score)</title><link>https://maku.blog/p/9dveh3m/</link><pubDate>Sun, 21 Apr 2024 00:00:00 +0000</pubDate><guid>https://maku.blog/p/9dveh3m/</guid><description>混同行列 (confusion matrix) とは 混同行列 (confusion matrix) は、機械学習やパターン認識などの分野で使われる評価手法のひとつで、主に分類問題の性能を評価するために使用されます。 混同行列は、実際のデータが属するクラスと、モデル（分類器）が予測したクラスを比較することで、分類器の性能を可視化するのに役立ちます。
図: 混同行列 混同行列は、予測モデルが予測したデータを下記の 4 種類に分けてカウントします。 陽性・陰性というのは、例えば、「病気の有無」や「本物・偽物」といった分類を示します。
真陽性 (TP: True Positives) &amp;hellip; 陽性データのうち、モデルが正しく陽性と予測した数。 真陰性 (TN: True Negatives) &amp;hellip; 陰性データのうち、モデルが正しく陰性と予測した数。 偽陽性 (FP: False Positives) &amp;hellip; 陰性データのうち、モデルが誤って陽性と予測した数。 偽陰性 (FN: False Negatives) &amp;hellip; 陽性データのうち、モデルが誤って陰性と予測した数。 一般的には二値分類に使われますが、多クラスの分類でも利用されます。 混同行列をベースにして、分類問題の様々な評価指標を計算できます。
正答率 (accuracy) と誤答率 (error rate) 再現率 (recall) と 適合率 (precision) F1 スコア、Fβ スコア マシューズ相関係数 (MCC) ここで、正答率 (accuracy) というのは、単純に「予測が当たった数／データ数」という計算で求められるものです。 例えば、10 個のデータを分類して、5 個のデータを正しく分類できれば、正答率 (accuracy) は50％です。
でも、実世界の分類問題はこんな簡単に評価できるものではなかったりします。 典型的なのが疾患の有無の判断です。 実際に 100 人中 5 人が疾患を持っている場合、誰も疾患を持っていないと予測すれば、その予測モデルの精度は95％になります。 でもそんな予測モデルは役に立ちませんよね？ そうではなく、いかに間違いなく疾患を持っている人を見つけられるかが重要なはずです。 そこで必要になってくるのが、再現率、適合率、F1 スコアといった評価指標です。</description></item><item><title>機械学習メモ: 回帰タスクの評価 ─ MAE, MSE, RMSE, MSLE, RMSLE, MAPE, 決定係数</title><link>https://maku.blog/p/psotadn/</link><pubDate>Tue, 06 May 2014 00:00:00 +0000</pubDate><guid>https://maku.blog/p/psotadn/</guid><description>機械学習における回帰モデルは、目的変数となる数値を予測するためのモデルです。 よって、回帰モデルの精度は、真の値と予測値のズレをもとに計算されます（ズレが小さいほど精度がよい）。 その計算方法としては、単純にズレの平均値を求める方法（平均絶対誤差: MAE）や、二乗値や対数値を使う方法などいろいろあります。
平均絶対誤差 (MAE: Mean Absolute Error) 平均絶対誤差 (MAE: Mean Absolute Error) は、実測値と予測値の「誤差の絶対値」をもとに算出します。
$$ \text{MAE} = \frac{1}{n} \sum_{i=1}^{n} |y_i - \hat{y}_i| $$
\( y_i \) : 実測値 \( \hat{y_i} \) : 予測値 二乗誤差に比べると、外れ値の影響が小さいのが特徴すす。
自力で計算する方法 import numpy as np import pandas as pd y_true = pd.Series([1.0, 2.0, 3.0, 4.0, 5.0]) # 実測値 y_pred = pd.Series([0.8, 2.4, 2.5, 3.8, 7.2]) # 予測値 print(np.mean(np.abs(y_true - y_pred))) #=&amp;gt; 0.7 scikit-learn の mean_absolute_error 関数を使う方法 import numpy as np import pandas as pd from sklearn.</description></item></channel></rss>